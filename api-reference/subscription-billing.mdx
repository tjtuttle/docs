---
title: 'Subscription & Billing'
description: 'Manage your subscription tier, monitor usage, and handle billing'
---

## Subscription Tiers

CTGT API offers two tiers designed for different needs:

<CardGroup cols={2}>
  <Card title="Free Tier" icon="gift">
    **Perfect for getting started**
    
    - 3 AI models
    - 20 req/min, 100 req/hour
    - 500 req/day
    - 100K tokens/day
    - Pay-as-you-go 
    - No credit card required
  </Card>
  
  <Card title="Paid Tier" icon="crown">
    **For production applications**
    
    - All 10 AI models
    - 100 req/min, 1,000 req/hour
    - 10,000 req/day
    - 10M tokens/day
    - Pay-as-you-go
    - Priority support
  </Card>
</CardGroup>

---

## Rate Limits Comparison

| Limit | Free Tier | Paid Tier | Increase |
|-------|-----------|-----------|----------|
| **Requests per minute** | 20 | 100 | **5x** |
| **Requests per hour** | 100 | 1,000 | **10x** |
| **Requests per day** | 500 | 10,000 | **20x** |
| **Tokens per day** | 100,000 | 10,000,000 | **100x** |
| **Available models** | 3 | 10 | **3.6x** |

<Note>
Rate limits reset at the start of each time period (minute, hour, day).
</Note>

---

## Rate Limit Headers

Every API response includes rate limit information in the headers:

```
X-RateLimit-Limit: 100
X-RateLimit-Remaining: 95
X-RateLimit-Reset: 1702847523
```

**Headers explained:**
- `X-RateLimit-Limit`: Maximum requests allowed in the current window
- `X-RateLimit-Remaining`: Requests remaining in current window
- `X-RateLimit-Reset`: Unix timestamp when the limit resets

### Example: Checking Rate Limits

```python
import requests

response = requests.post(
    "https://api.ctgt.ai/v1/chat/completions",
    headers={"Authorization": f"Bearer {api_key}"},
    json={"model": "gemini-2.5-flash", "messages": [...]}
)

print(f"Limit: {response.headers.get('X-RateLimit-Limit')}")
print(f"Remaining: {response.headers.get('X-RateLimit-Remaining')}")
print(f"Resets at: {response.headers.get('X-RateLimit-Reset')}")
```

---

## Handling Rate Limits

When you exceed your rate limits:

**Status Code:** `429 Too Many Requests`

**Response:**
```json
{
  "detail": "Rate limit exceeded. Please try again later."
}
```

### Best Practices

<AccordionGroup>
  <Accordion title="Implement Exponential Backoff">
    ```python
    import time
    import requests
    
    def make_request_with_retry(url, headers, data, max_retries=5):
        for attempt in range(max_retries):
            response = requests.post(url, headers=headers, json=data)
            
            if response.status_code == 429:
                wait_time = 2 ** attempt  # Exponential backoff
                print(f"Rate limited. Waiting {wait_time}s...")
                time.sleep(wait_time)
                continue
            
            return response
        
        raise Exception("Max retries exceeded")
    ```
  </Accordion>
  
  <Accordion title="Monitor Your Rate Limit Headers">
    ```python
    def monitor_rate_limits(response):
        remaining = int(response.headers.get('X-RateLimit-Remaining', 0))
        
        if remaining < 10:
            print(f"Warning: Only {remaining} requests remaining!")
        
        if remaining == 0:
            reset_time = int(response.headers.get('X-RateLimit-Reset', 0))
            wait_seconds = reset_time - time.time()
            print(f"Rate limit exhausted. Waiting {wait_seconds}s")
            time.sleep(wait_seconds)
    ```
  </Accordion>
  
  <Accordion title="Cache Responses When Possible">
    ```python
    from functools import lru_cache
    import hashlib
    import json
    
    @lru_cache(maxsize=1000)
    def cached_api_call(prompt_hash):
        # Make actual API call
        response = requests.post(...)
        return response.json()
    
    def get_completion(prompt):
        # Create hash of prompt for caching
        prompt_hash = hashlib.md5(prompt.encode()).hexdigest()
        return cached_api_call(prompt_hash)
    ```
  </Accordion>
  
  <Accordion title="Batch Similar Requests">
    ```python
    # Instead of 100 individual requests
    for item in items:
        result = api_call(item)
    
    # Batch into fewer requests with multiple items
    for batch in chunks(items, 10):
        results = api_call_batch(batch)
    ```
  </Accordion>
</AccordionGroup>

<Tip>
**Pro tip:** Upgrade to paid tier for 5-100x higher rate limits if you're consistently hitting limits.
</Tip>

---

### Optimize Costs

<CardGroup cols={2}>
  <Card title="Choose the Right Model" icon="bullseye">
    Use cheaper models for simple tasks:
    - Gemini Flash Lite: $0.30 input
    - GPT-5 Nano: $0.25 input
  </Card>
  
  <Card title="Control Token Limits" icon="sliders">
    Set `max_tokens` to limit response length:
    ```json
    {"max_tokens": 500}
    ```
  </Card>
  
  <Card title="Optimize Prompts" icon="message">
    Shorter prompts = lower costs:
    - Be concise
    - Remove unnecessary context
    - Keep messages focused
  </Card>
  
  <Card title="Cache Common Responses" icon="database">
    Store and reuse responses for:
    - FAQ answers
    - Common queries
    - Static content
  </Card>
</CardGroup>

---

## Pricing Summary

### Pay-as-you-go Pricing

Both tiers pay for token usage at the same rates:

| Model Category | Input (per 1M) | Output (per 1M) |
|----------------|----------------|-----------------|
| **Most Affordable** | $0.25 - $0.50 | $0.60 - $2.70 |
| **Mid-Range** | $1.20 - $4.00 | $5.20 - $14.00 |
| **Premium** | $5.00 - $10.00 | $17.00 - $30.00 |

<Info>
See the [Models & Pricing](/api-reference/models-pricing) page for complete pricing details.
</Info>

---

## Example Cost Scenarios

### Scenario 1: Small Project (Free Tier)

**Usage:**
- 500 requests/day
- Average 100 input + 300 output tokens per request
- Using Gemini 2.5 Flash

**Monthly Cost:**
- Input: 500 × 30 × 100 tokens = 1.5M tokens = $0.75
- Output: 500 × 30 × 300 tokens = 4.5M tokens = $12.15
- **Total: $12.90/month**

### Scenario 2: Medium Project (Paid Tier)

**Usage:**
- 5,000 requests/day
- Average 200 input + 500 output tokens per request
- Mix of Gemini Flash and GPT-5

**Monthly Cost:**
- Usage: ~$150-200
- **Total: $150-200/month**

### Scenario 3: Large Project (Paid Tier)

**Usage:**
- 50,000 requests/day
- Using advanced models (Claude Sonnet, GPT-5.2)
- Complex queries with higher token counts

**Monthly Cost:**
- Usage: ~$1,500-2,500
- **Total: $1,500-2,500/month**

<Note>
All scenarios assume normal usage patterns. Your costs may vary based on actual token consumption.
</Note>

---

## Next Steps

<CardGroup cols={2}>
  <Card title="View All Models" icon="layer-group" href="/api-reference/models-pricing">
    Compare all 10 models and their pricing
  </Card>
  
  <Card title="API Features" icon="wand-magic-sparkles" href="/api-reference/api-features">
    Learn about streaming and advanced features
  </Card>
  
  <Card title="Code Examples" icon="code" href="/api-reference/code-examples">
    See complete SDK implementations
  </Card>
  
  <Card title="Chat Completions" icon="messages" href="/api-reference/endpoint/chat-completions">
    Full API reference documentation
  </Card>
</CardGroup>