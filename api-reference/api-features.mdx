---
title: 'Advanced Features'
description: 'Unlock the full power of the CTGT API with streaming, custom prompts, and fine-tuning controls'
---

## Overview

The CTGT API offers advanced features to customize and optimize your AI interactions:

<CardGroup cols={2}>
  <Card title="Streaming Responses" icon="water">
    Real-time token-by-token output
  </Card>
  
  <Card title="Token Limits" icon="ruler">
    Control response length and costs
  </Card>
</CardGroup>

---

## Streaming Responses

Get responses in real-time as they're generated, similar to ChatGPT's typing effect.

### Enable Streaming

Set `stream: true` in your request:

<CodeGroup>

```bash cURL
curl -X POST https://api.ctgt.ai/v1/chat/completions \
  -H "Authorization: Bearer sk-ctgt-YOUR_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gemini-2.5-flash",
    "messages": [
      {"role": "user", "content": "Write a short story"}
    ],
    "stream": true
  }'
```

```python Python
import requests
import json

api_key = "sk-ctgt-YOUR_API_KEY"
url = "https://api.ctgt.ai/v1/chat/completions"

headers = {
    "Authorization": f"Bearer {api_key}",
    "Content-Type": "application/json"
}

data = {
    "model": "gemini-2.5-flash",
    "messages": [
        {"role": "user", "content": "Write a short story"}
    ],
    "stream": True
}

response = requests.post(url, headers=headers, json=data, stream=True)

for line in response.iter_lines():
    if line:
        line = line.decode('utf-8')
        if line.startswith('data: '):
            data_str = line[6:]  # Remove 'data: ' prefix
            if data_str == '[DONE]':
                break
            try:
                chunk = json.loads(data_str)
                content = chunk['choices'][0]['delta'].get('content', '')
                print(content, end='', flush=True)
            except json.JSONDecodeError:
                pass
print()
```

```javascript JavaScript
const response = await fetch('https://api.ctgt.ai/v1/chat/completions', {
  method: 'POST',
  headers: {
    'Authorization': 'Bearer sk-ctgt-YOUR_API_KEY',
    'Content-Type': 'application/json'
  },
  body: JSON.stringify({
    model: 'gemini-2.5-flash',
    messages: [
      { role: 'user', content: 'Write a short story' }
    ],
    stream: true
  })
});

const reader = response.body.getReader();
const decoder = new TextDecoder();

while (true) {
  const { done, value } = await reader.read();
  if (done) break;
  
  const chunk = decoder.decode(value);
  const lines = chunk.split('\n');
  
  for (const line of lines) {
    if (line.startsWith('data: ')) {
      const data = line.slice(6);
      if (data === '[DONE]') break;
      try {
        const parsed = JSON.parse(data);
        const content = parsed.choices[0].delta.content || '';
        process.stdout.write(content);
      } catch (e) {}
    }
  }
}
```

</CodeGroup>

### Streaming Response Format

**Server-Sent Events (SSE):**
```
data: {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1702847123,"model":"gemini-2.5-flash","choices":[{"index":0,"delta":{"content":"Once"},"finish_reason":null}]}

data: {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1702847123,"model":"gemini-2.5-flash","choices":[{"index":0,"delta":{"content":" upon"},"finish_reason":null}]}

data: {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1702847123,"model":"gemini-2.5-flash","choices":[{"index":0,"delta":{"content":" a"},"finish_reason":null}]}

...

data: [DONE]
```

### When to Use Streaming

<CardGroup cols={2}>
  <Card title="Interactive Applications" icon="messages">
    Chat interfaces, chatbots, conversational UIs
  </Card>
  
  <Card title="Long-Form Content" icon="book">
    Articles, reports, stories, documentation
  </Card>
  
  <Card title="Better UX" icon="face-smile">
    Show progress, reduce perceived latency
  </Card>
  
  <Card title="Real-Time Feedback" icon="bolt">
    Users see responses immediately
  </Card>
</CardGroup>

<Tip>
Streaming is ideal for user-facing applications where showing incremental progress improves the experience.
</Tip>


---

## Token Limits

Control response length and manage costs with `max_tokens`.

### Setting Token Limits

<CodeGroup>

```bash Short Response (100 tokens)
curl -X POST https://api.ctgt.ai/v1/chat/completions \
  -H "Authorization: Bearer sk-ctgt-YOUR_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gemini-2.5-flash",
    "messages": [
      {"role": "user", "content": "Summarize quantum physics"}
    ],
    "max_tokens": 100
  }'
```

```python Medium Response (500 tokens)
data = {
    "model": "gemini-2.5-flash",
    "messages": [
        {"role": "user", "content": "Explain machine learning"}
    ],
    "max_tokens": 500
}
```

```javascript Long Response (2000 tokens)
const data = {
  model: 'gemini-2.5-flash',
  messages: [
    { role: 'user', content: 'Write a detailed essay on AI ethics' }
  ],
  max_tokens: 2000
};
```

</CodeGroup>

### Token Guidelines

| Tokens | Approximate Length | Best For |
|--------|-------------------|----------|
| **50-100** | 1-2 short paragraphs | Quick answers, summaries |
| **200-500** | 1-2 medium paragraphs | Explanations, descriptions |
| **500-1000** | 1-2 pages | Detailed responses, articles |
| **1000-2000** | 2-4 pages | Long-form content, essays |
| **2000+** | Multiple pages | Reports, documentation |

<Note>
**1 token ≈ 4 characters** or **¾ of a word** in English.
</Note>

### Cost Impact

```python
# Example: 1000-token response
# Gemini 2.5 Flash: $0.0027
# Claude Sonnet 4.5: $0.017
# Claude Opus 4.5: $0.030

# Setting max_tokens=200 reduces costs by 80%
```


---

## Combining Features

### Example: Production Chat Application

```python
import requests
import json

def chat_completion(
    api_key: str,
    user_message: str,
    conversation_history: list = None,
    model: str = "gemini-2.5-flash",
    streaming: bool = True,
    temperature: float = 0.7,
    max_tokens: int = 500
):
    """
    Complete chat request with all advanced features
    """
    url = "https://api.ctgt.ai/v1/chat/completions"
    
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    
    # Build messages with history
    messages = []
    
    if conversation_history:
        messages.extend(conversation_history)
    
    messages.append({
        "role": "user",
        "content": user_message
    })
    
    # Request payload
    data = {
        "model": model,
        "messages": messages,
        "stream": streaming,
        "max_tokens": max_tokens
    }
    
    response = requests.post(url, headers=headers, json=data, stream=streaming)
    
    if streaming:
        # Handle streaming response
        full_response = ""
        for line in response.iter_lines():
            if line:
                line = line.decode('utf-8')
                if line.startswith('data: '):
                    data_str = line[6:]
                    if data_str == '[DONE]':
                        break
                    try:
                        chunk = json.loads(data_str)
                        content = chunk['choices'][0]['delta'].get('content', '')
                        full_response += content
                        print(content, end='', flush=True)
                    except json.JSONDecodeError:
                        pass
        print()
        return full_response
    else:
        # Handle non-streaming response
        result = response.json()
        return result['choices'][0]['message']['content']

# Usage
conversation = []

response1 = chat_completion(
    api_key="sk-ctgt-YOUR_API_KEY",
    user_message="What is machine learning?",
    conversation_history=conversation,
    streaming=True
)

conversation.append({"role": "user", "content": "What is machine learning?"})
conversation.append({"role": "assistant", "content": response1})

response2 = chat_completion(
    api_key="sk-ctgt-YOUR_API_KEY",
    user_message="Give me a code example",
    conversation_history=conversation,
    streaming=True
)
```

---

## Best Practices

<CardGroup cols={2}>
  <Card title="Set Token Limits" icon="ruler">
    - Prevent excessive costs
    - Control response length
    - Match your UI constraints
  </Card>
  
  <Card title="Enable Streaming" icon="water">
    - Better user experience
    - Show progress in real-time
    - Ideal for chat interfaces
  </Card>
  
  <Card title="Monitor Usage" icon="chart-line">
    - Track token consumption
    - Set budget alerts
    - Optimize costs regularly
  </Card>
  
  <Card title="Handle Errors" icon="shield-check">
    - Implement retry logic
    - Use exponential backoff
    - Log all failures
  </Card>
</CardGroup>

### Error Handling

```python
def safe_api_call(api_key, messages, max_retries=3):
    for attempt in range(max_retries):
        try:
            response = requests.post(
                "https://api.ctgt.ai/v1/chat/completions",
                headers={"Authorization": f"Bearer {api_key}"},
                json={
                    "model": "gemini-2.5-flash",
                    "messages": messages,
                    "max_tokens": 500
                },
                timeout=30
            )
            
            if response.status_code == 200:
                return response.json()
            elif response.status_code == 429:
                # Rate limit - wait and retry
                time.sleep(2 ** attempt)
                continue
            else:
                print(f"Error: {response.status_code}")
                return None
                
        except requests.exceptions.Timeout:
            print(f"Timeout on attempt {attempt + 1}")
            if attempt < max_retries - 1:
                continue
        except Exception as e:
            print(f"Error: {e}")
            return None
    
    return None
```

---

## Next Steps

<CardGroup cols={2}>
  <Card title="Code Examples" icon="code" href="/api-reference/code-examples">
    See complete SDK implementations
  </Card>
  
  <Card title="Models & Pricing" icon="layer-group" href="/api-reference/models-pricing">
    Choose the right model for your needs
  </Card>
  
  <Card title="Subscription & Billing" icon="credit-card" href="/api-reference/subscription-billing">
    Manage your subscription and usage
  </Card>
  
  <Card title="Chat Completions API" icon="messages" href="/api-reference/endpoint/chat-completions">
    Full API reference
  </Card>
</CardGroup>